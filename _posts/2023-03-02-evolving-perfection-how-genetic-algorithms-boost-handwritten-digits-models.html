---
layout: post
title: 'Evolving Perfection: How Genetic Algorithms Boost Handwritten Digits Models'
canonical_url: https://python.plainenglish.io/evolving-perfection-how-genetic-algorithms-boost-handwritten-digits-models-44efbdcc5acf?source=rss-1d94e45b115------2
tag:
- optimization
- machine-learning
- genetic-algorithm
- mnist
- handwritten-digit
---

<h4>Can nature-inspired algorithms lead us to the ultimate handwritten digits recognizer?</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/520/1*w-i9TJnIQ6pg9UKYrWWSUA.jpeg" /><figcaption>Photo by <a href="https://www.shutterstock.com/g/Gleb+Kadeev">Uncle Leo</a> on <a href="https://www.shutterstock.com/image-vector/theory-evolution-man-silhouette-human-development-772681819">Shutterstock</a></figcaption></figure><p>In the realm of machine learning, the task of recognizing handwritten digits has been a benchmark problem for decades. Various techniques are being explored by researchers to enhance the accuracy of these models, but achieving optimal performance is still a challenging task. Genetic algorithms, a class of nature-inspired optimization techniques, offer a promising approach to finding the best set of hyperparameters for these models.</p><p>The article delves into the workings of genetic algorithms and their application for optimizing a handwritten digits CNN model. By the end of the article, readers will gain insight into the potential of genetic algorithms in model optimization and the errors that they can face during its development and be equipped to use this technique in their own machine learning projects.</p><h4>How are Genetic Algorithms(GAs) incorporated into a CNN</h4><p>Traditionally, hyperparameters are tuned manually or using grid search, which can be time-consuming and often results in suboptimal performance.</p><p>Genetic algorithms can be utilized to optimize the hyperparameters of a two-layer CNN for recognizing handwritten digits in this project. In this project, various combinations of <strong>filter sizes</strong> and <strong>kernel sizes</strong> for the two convolutional layers were explored to identify the best combination that maximizes the accuracy of the model on the MNIST dataset.</p><h3>Stage 1: Developing the CNN</h3><p>The architecture used in the genetic algorithm is a 2-layer convolutional neural network followed by a fully connected layer and an output layer with 10 outputs. The convolutional layers have random filter and kernel sizes, which are determined by the genetic algorithm during the optimization process.</p><p>The Network class shown below is inherited from the nn.Module class and contains these parameters:<br>1. <strong>channels</strong>: The randomly generated filter sizes for each convolution layer.<br>2. <strong>kSizes</strong>: The randomly generated kernel sizes for each convolution layer<br>3. <strong>stride</strong>: The stride for each convolution layer<br>4. <strong>padding</strong>: The padding for each convolution layer<br>5. <strong>output_flatten</strong>: This is an integer equal to the final output shape from the convolutions that is calculated by a function which is discussed in detail <a href="#6c53">below</a>.</p><pre>import torch.nn as nn<br><br>class Network(nn.Module):<br>    def __init__(self, channels, kSizes, stride, padding, pool, output_flatten):<br>        super().__init__()</pre><p>In the code below, the first convolutional layer applies a set of filters to the input image to extract low-level features such as edges and corners. The filter and kernel sizes in this layer are randomly selected by the genetic algorithm from a predefined range of values.</p><pre>self.layer1 = nn.Sequential(<br>            nn.Conv2d(in_channels=1, out_channels=channels[0],<br>                      kernel_size=kSizes[0], stride=stride, padding=padding),<br>            nn.BatchNorm2d(num_features=channels[0]),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=pool, stride=stride)<br>        )</pre><p>The output of the first convolutional layer is passed through a ReLU activation function and then max-pooled to reduce the dimensionality of the feature maps.</p><p>The second convolutional layer applies a second set of filters to the output of the first layer to extract more complex features such as shapes and patterns. The filter and kernel sizes in this layer are also randomly selected by the genetic algorithm from a predefined range of values.</p><pre>self.layer2 = nn.Sequential(<br>            nn.Conv2d(in_channels=channels[0], out_channels=channels[1],<br>                      kernel_size=kSizes[1], stride=stride, padding = padding),<br>            nn.BatchNorm2d(num_features=channels[1]),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=pool, stride=stride)<br>        )</pre><p>The output of the second convolutional layer is again passed through a ReLU activation function and then max-pooled to reduce the dimensionality of the feature maps.</p><p>The resulting feature maps are then flattened and passed through a fully connected layer, which combines the features to produce a final classification output.</p><pre>self.fc = nn.Sequential(<br>            nn.Linear(in_features=output_flatten, out_features=256),<br>            nn.Dropout(p=0.3),<br>            nn.ReLU()<br>        )<br>self.out = nn.Linear(in_features=256, out_features=10) #10 digits</pre><p>The CNN was the first step of this project. The next step involves integrating the CNN with the Genetic algorithm.</p><h3>Stage 2: The Genetic Algorithm</h3><p>Here’s an explanation of how genetic algorithms work using the five key phases, using the example of optimizing a handwritten digit recognition model:</p><pre>class Genetic:<br>    def __init__(self,pop_size,nlayers,max_nfilters,max_sfilters):<br>        self.pop_size = pop_size<br>        self.nlayers = nlayers<br>        self.max_nfilters = max_nfilters<br>        self.max_sfilters = max_sfilters<br>        self.max_acc = 0<br>        self.best_arch = np.zeros((1,6))<br>        self.gen_acc = []<br>        self.best_model = None</pre><h4><strong>Initial Population</strong></h4><p>The first step in a genetic algorithm is to create an initial population of <strong>individuals</strong>, each of which represents a potential solution to the problem. In this project, this could mean randomly generating a set of initial hyperparameters for the model. In this case: <br><strong>(a)</strong>the number of filters<br><strong>(b)</strong> the size of the kernel in the convolution</p><pre> def generate_population(self):<br>        np.random.seed(0)<br>        pop_nlayers = np.random.randint(1,self.max_nfilters,(self.pop_size,self.nlayers))<br>        pop_sfilters = np.random.randint(1,self.max_sfilters,(self.pop_size,self.nlayers))<br>        pop_total = np.concatenate((pop_nlayers,pop_sfilters),axis=1)<br>        return pop_total</pre><p>Here, the population will consist of individuals that are a randomly generated array of filter size and kernel sizes. This is an example of a population:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/178/1*ygrwk7cP5OGamYymJ9Ogng.png" /><figcaption>Image by Author</figcaption></figure><p>The first two elements in the array are the filter sizes for the two convolutions. The last two elements are the kernel sizes respectively.</p><h4><strong>Fitness Function</strong></h4><p>The fitness function evaluates the performance of each individual in the population by measuring how well its solution performs on the given problem. In the case of a handwritten digit recognition model, the fitness function could be each individual model&#39;s accuracy on a test dataset. Individuals with higher accuracy scores will have a higher fitness value.</p><pre>def calc_accuracy(test_loader, model, pop_acc):<br>  for images, labels in tqdm(test_loader):<br>    with torch.no_grad():<br>      output = model(images.cuda())<br>    predictions = torch.argmax(output, dim=1)<br>    correct += torch.sum((predictions.cpu() == labels).float())<br>  acc = correct/total<br>  pop_acc.append(acc*100)</pre><p>The calc_accuracy() function will be used to find out the accuracy of each individual’s fitness.</p><h4><strong>Selection</strong></h4><p>Once the fitness values of each individual have been determined, a selection process is used to choose which individuals will be selected for the next generation.</p><pre>def select_parents(self,pop,nparents,fitness):<br>        parents = np.zeros((nparents,pop.shape[1]))<br>        for i in range(nparents):<br>            best = np.argmax(fitness)<br>            parents[i] = pop[best]<br>            fitness[best] = -99999<br>        return parents</pre><p>The function select_parents() will essentially select individuals from the population that had better fitness compared to others. If nparents was 5, then the 5 fittest individuals would be selected from the population. In other words, the top 5 models with respect to their accuracies would be selected for the next step.</p><h4><strong>Crossover</strong></h4><p>In the crossover phase, pairs of selected individuals are combined to create new individuals, known as offspring. This is done by randomly selecting points in each individual’s parameter configuration and swapping the values between them.</p><pre>def crossover(self, parents):<br>        nchild = self.pop_size - parents.shape[0]<br>        nparents = parents.shape[0]<br>        child = np.zeros((nchild,parents.shape[1]))<br>        for i in range(nchild):<br>            first = i % nparents<br>            second = (i+1) % nparents<br>            child[i,:1] = parents[first][:1]<br>            child[i,1] = parents[second][1]<br>            child[i,2:3] = parents[first][2:3]<br>            child[i,3] = parents[second][3]<br>        return child</pre><p>In this case, the number of children created is equal to the difference between the population size and the number of selected parents. This helps in keeping the population size constant. In this function, two consecutive parents are chosen as a pair(first and second). The offspring contains:<br><strong>(a)</strong> The first parent’s first filter size and kernel size.<br><strong>(b)</strong> The second parent’s second filter size and kernel size.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/933/1*GJhhNATeGn8ERaxtvtdrZQ.png" /><figcaption>Image by Author</figcaption></figure><p>The image above displays a crossover of two, parent individuals to form two offspring.</p><h4><strong>Mutation</strong></h4><p>Finally, a mutation process is introduced to the offspring population to introduce new genetic diversity. This involves randomly changing one or more parameters of an individual in the offspring population. This could mean randomly adjusting the filter or kernel sizes in the layers for a handwritten digit recognition model.</p><pre> def mutation(self,child):<br>        for i in range(child.shape[0]):<br>            val = np.random.randint(1,20)<br>            ind = np.random.randint(1,3) - 1<br>            if child[i][ind] + val &gt; 100:<br>                child[i][ind] -= val<br>            else:<br>                child[i][ind] += val<br>            val = np.random.randint(1,5)<br>            ind = np.random.randint(3,5) - 1<br>            print(ind)<br>            if child[i][ind] + val &gt; 10:<br>                child[i][ind] -= val<br>            else:<br>                child[i][ind] += val<br>        return child</pre><p>In the mutation() function, one of the filter sizes and the kernel sizes are randomly tuned for <strong>each</strong> <strong>offspring</strong> in the new population.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/888/1*P5H1cqY-vK6jSar7vNZnGw.png" /><figcaption>Image by Author</figcaption></figure><p>Here randomly chosen values are added to randomly chosen filter sizes and kernel sizes of the offspring. In the figure, <strong>-2</strong> is subtracted from <strong>9</strong> in Child 2 as in the code we have set a limit of <strong>10</strong> for the kernel size.</p><pre>&quot;&quot;&quot;<br>The below code is from the mutation() function<br>&quot;&quot;&quot;<br><br>if child[i][ind] + val &gt; 10:<br>  child[i][ind] -= val<br>else:<br>  child[i][ind] += val</pre><p>As <strong>9+2</strong> is greater than 10, we execute <strong>9–2</strong>.</p><p>These five phases are repeated for multiple generations until a stopping criterion is met, such as reaching a certain level of accuracy, or in this case, the maximum number of generations set as 10. By using genetic algorithms to optimize the hyperparameters of a handwritten digit recognition model, we can improve its accuracy and potentially achieve state-of-the-art performance.</p><h3>Solving the inconsistent matrix shape multiplication error — the output_flatten parameter</h3><p>As the filter and kernel size is random, the output shape of the matrix from the two convolution layers also becomes random. This produces an error while training. <br>An example error produced during training:<br>RuntimeError: mat1 and mat2 shapes cannot be multiplied (100x612 and 1728x256) .</p><p>To resolve this problem, an understanding was gained of how the output shape of each convolution is computed, hence the following code was produced:</p><pre>from math import floor<br>def calculate_convolution_output_shape(input_shape, filter_shape, stride, padding, pool, nfilters=1, last_conv=False):<br>    <br>    height = (input_shape[0] - filter_shape + (2 * padding)) // stride + 1<br>    width = (input_shape[1] - filter_shape + (2 * padding)) // stride + 1<br><br>    height = floor(height / pool)<br>    width = floor(width / pool)<br>    # Calculate the output shape of the convolutional layer<br>    output_shape = (height, width)<br>    if last_conv:<br>        output_flatten = height*width*nfilters<br>        return output_flatten<br>    else:<br>        return output_shape</pre><p>The explanation of the mathematics behind this can be found <a href="https://iq.opengenus.org/output-size-of-convolution/">here</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/727/1*YizLDNK_uWENyhr6YaIDMg.png" /><figcaption><strong>Powered By Embed Fun </strong>| (<strong><em>Left</em></strong>) Formula for calculating Output Height | (<strong>Right</strong>) Formula for calculating Output Width |</figcaption></figure><p>H’: Output height, H: Input height, W’: Output width, W: Output width, K: Kernel size, P: Padding, S: Stride</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fmath.embed.fun%2Fembed%2FjZvyPcoxE5RhP5Rfpsv6Gv&amp;display_name=Embed+Fun&amp;url=https%3A%2F%2Fmath.embed.fun%2FjZvyPcoxE5RhP5Rfpsv6Gv&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=embed" width="221" height="158" frameborder="0" scrolling="no"><a href="https://medium.com/media/87b27d89fed2021dced36eea8ccbce5c/href">https://medium.com/media/87b27d89fed2021dced36eea8ccbce5c/href</a></iframe><p>Using the above formulae, the final output height (H’’) and width (W’’) are calculated. Upon reaching the last convolution (last_conv = True), the height, width, and last filter size are multiplied.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fmath.embed.fun%2Fembed%2Fb1WxWCJG2xYXAvAwiabE7V&amp;display_name=Embed+Fun&amp;url=https%3A%2F%2Fmath.embed.fun%2Fb1WxWCJG2xYXAvAwiabE7V&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=embed" width="343" height="137" frameborder="0" scrolling="no"><a href="https://medium.com/media/014611eb68a46c094132f3c00c327f33/href">https://medium.com/media/014611eb68a46c094132f3c00c327f33/href</a></iframe><p>For every individual the output_flatten parameter is calculated by calling the calculate_convolution_output_shape() function in the fitness() function of the Network which finally started the training successfully.</p><h4>Results and Conclusion</h4><p>After training with:</p><pre>pop_size = 10<br>num_generations = 5<br>epochs = 5<br>nparents = 5</pre><p>The model reached an accuracy of<strong> 96.63% </strong>with only <strong>5</strong> <strong>generations</strong> and a <strong>population size of 10</strong>. It is expected that with an increase in the number of generations, surely better results can be achieved.</p><p>There are other ways through which this project can be modified. Instead of changing the convolution parameters, other hyperparameters such as learning rate, batch size, etc can be changed. Taking weights from each individual model is another method through which genetic algorithms can be applied.</p><p>In conclusion, the project demonstrates the effectiveness of using a genetic algorithm to optimize the hyperparameters of a two-layer convolutional neural network for handwritten digit recognition. By exploring different combinations of filter sizes and kernel sizes, the optimal configuration that achieved high accuracy was found on the MNIST dataset. The success of this project highlights the potential of using genetic algorithms for hyperparameter optimization in deep learning, which can lead to improved performance and more efficient model training.</p><p>Hope you were able to learn something new from this article. Please text me on my socials or in the comments below if you come across any doubts, errors, or suggestions.</p><p>The <strong>GitHub repository</strong> containing the working python files can be found here:</p><p><a href="https://github.com/pranavgupta2603/genetic-handwritten-digits">GitHub - pranavgupta2603/genetic-handwritten-digits</a></p><h4>References</h4><ul><li><a href="https://medium.com/analytics-vidhya/mnist-classifier-using-genetic-cnn-e1e860ecc2e9">MNIST Classifier using Genetic CNN</a></li><li><a href="https://sainivedh.medium.com/optimization-of-cnn-architecture-using-genetic-algorithm-for-image-classification-5c48f25dac9c">Optimization of CNN Architecture using Genetic Algorithm for Image Classification</a></li></ul><p><em>More content at </em><a href="https://plainenglish.io/"><strong><em>PlainEnglish.io</em></strong></a><em>. Sign up for our </em><a href="http://newsletter.plainenglish.io/"><strong><em>free weekly newsletter</em></strong></a><em>. Join our </em><a href="https://discord.gg/GtDtUAvyhW"><strong><em>Discord</em></strong></a><em> community and follow us on </em><a href="https://twitter.com/inPlainEngHQ"><strong><em>Twitter</em></strong></a>, <a href="https://www.linkedin.com/company/inplainenglish/"><strong><em>LinkedIn</em></strong></a><em> and</em><strong><em> </em></strong><a href="https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw"><strong><em>YouTube</em></strong></a><strong><em>.</em></strong></p><p><strong><em>Learn how to build awareness and adoption for your startup with </em></strong><a href="https://circuit.ooo/?utm=publication-post-cta"><strong><em>Circuit</em></strong></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=44efbdcc5acf" width="1" height="1" alt=""><hr><p><a href="https://python.plainenglish.io/evolving-perfection-how-genetic-algorithms-boost-handwritten-digits-models-44efbdcc5acf">Evolving Perfection: How Genetic Algorithms Boost Handwritten Digits Models</a> was originally published in <a href="https://python.plainenglish.io">Python in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
